{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A particular challenge with sequential data and modelling tasks is that the sequence lengths can vary from one dataset example to the next. This makes the use of a fixed input size architecture such as the MLP unsuitable. In addition, there can be many different types of sequential modelling tasks that we want to consider, each of which could have different architectural requirements, not just one-to-one relation like MLP. For example:\n",
    "- Text sentiment analysis (many-to-one)\n",
    "- Image captioning (one-to-many)\n",
    "- Language translation (many-to-many)\n",
    "- Part-of-speech tagging (many-to-many)\n",
    "\n",
    "The recurrent neural network (RNN) is designed to handle this variability of lengths in sequence data and diversity of problem tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic RNN Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\{\\boldsymbol x_t\\}_{t=1}^T$ be an example sequence input, with each $\\boldsymbol x_t\\in\\mathbb R^D$. Suppose that we are in the many-to-many setting, and there is a corresponding sequence of labels $\\{y_t\\}_{t=1}^T$, with $y_t\\in Y$, where $Y$ could be $\\{0,1\\}$ for a binary classification task for example.\n",
    "\n",
    "The basic RNN computation is given as follows:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\boldsymbol h_t^{(1)}=\\sigma\\left( \\mathbf W_{hh}^{(1)}\\boldsymbol h_{t-1}^{(1)} + \\mathbf W_{xh}^{(1)}\\boldsymbol x_t + \\boldsymbol b_h^{(1)} \\right)\n",
    "\\\\\n",
    "\\hat {\\boldsymbol y}_t = \\sigma_{out}\\left( \\mathbf W_{hy}\\boldsymbol h_t^{(1)} +\\boldsymbol b_y \\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "for $t=1,\\cdots,T$, where $\\boldsymbol h_t^{(1)}\\in\\mathbb R^{n_1}$, $\\mathbf W_{hh}^{(1)}\\in\\mathbb R^{n_1\\times n_1}$, $\\mathbf W_{xh}^{(1)}\\in\\mathbb R^{n_1\\times D}$, $\\boldsymbol b_h^{(1)}\\in\\mathbb R^{n_1}$, $\\hat{\\boldsymbol y}_t\\in\\mathbb R^{n_y}$, $\\mathbf W_{hy}\\in\\mathbb R^{n_y\\times n_1}$, $\\boldsymbol b_y\\in\\mathbb R^{n_y}$, $\\sigma$ and $\\sigma_{out}$ are activation functions, $n_1$ is the number of units in the hidden layer, and $n_y$ is the dimension of the output space $Y$. Note that the computation requires an initial hidden state $\\boldsymbol h_0^{(1)}$ to be defined, although in practice this is often just set to the zero vector.\n",
    "\n",
    "![rnn-structure](../../figures/rnn-structure.png)\n",
    "\n",
    "Recurrent neural networks make use of weight sharing, similar to convolutional neural networks, but this time the weights are shared across time. This allows the RNN to be 'unrolled' for as many time steps as there are in the data input $\\boldsymbol x$.\n",
    "\n",
    "The RNN also has a persistent state, in the form of the hidden layer $\\mathbf h^{(1)}$. This hidden state can carry information over an arbitrary number of time steps, and so predictions at a given time step $t$ can depend on events that occurred at any point in the past, at least in principle. As with MLPs, the hidden state stores distributed representations of information, which allows them to store a lot of information, in contrast to hidden Markov models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs can also be made more powerful by stacking recurrent layers on top of each other:\n",
    "\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\boldsymbol h_t^{(k)} = \\sigma\\left( \\mathbf W_{hh}^{(k)}\\boldsymbol h_{t-1}^{(k)} + \\mathbf W_{xh}^{(k)} \\boldsymbol h_t^{(k-1)} + \\boldsymbol b_h^{(k)} \\right)\n",
    "\\\\\n",
    "\\hat y_t = \\sigma_{out}\\left( \\mathbf W_{hy} \\boldsymbol h^{(L)} + \\boldsymbol b_y \\right)\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "for $k=1,\\cdots,L$.\n",
    "\n",
    "![stacked-rnn-structure](../../figures/stacked_rnn_structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard recurrent neural networks are uni-directional. That is, they only take past context into account. In some applications, where the full input sequence is available to make predictions, it is possible and desirable for the network to take both past and future context into account.\n",
    "\n",
    "For example, consider a part-of-speech (POS) tagging problem, where the task is to label each word in a sentence according to its particular part of speech, e.g. noun, adjective, verb etc. In some cases, the correct label can be ambiguous given only the past context, for example the word \"light\" in the sentence \"There's a light...\" could be a adjective or a noun depending on how the sentence continues. \n",
    "\n",
    "Bidirectional RNNs are designed to look at both future and past context. They consist of two RNNs running forward and backwards in time, whose states are combined in sum way (e.g. adding or concatenating) to produce the final hidden state of the layer.\n",
    "\n",
    "![bidirectional-rnn-structure](../../figures/bidirectional_rnn-structure.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RNNs are trained in the same way as multilayer perceptrons and convolutional neural networks. A loss function $L(y_1,\\cdots, y_T, \\hat y_1,\\cdots, \\hat y_T) is defined according to the problem task and learning principle, and the network is trained using the backpropagation algorithm and a selected network optimiser. In the many-to-one case (e.g. sentiment analysis), the loss function may be defined as $L(y_T,\\hat y_T)$.\n",
    "\n",
    "Recall the equation describing the backpropagation of errors in the MLP case:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta^{(k)}=\\sigma'(\\boldsymbol a^{(k)})(\\mathbf W^{(k)})^T\\delta^{(k+1)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for $k=1,\\cdots,L$ where $k$ indexes the hidden layers. In the case of recurrent neural networks, the errors primarily backpropagate along the time direction, and we obtain the following propagation of errors in the hidden states:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_{t-1}^{(k)}=\\sigma'(\\boldsymbol a_{t-1}^{(k)})(\\mathbf W_{hh}^{(k)})^T\\delta_t^{(k)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "for $t=T,\\cdots,1$. For this reason, the backpropagation algorithm for RNNs is referred to as backpropagation through time (BPTT)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks can also be trained as generaive models for unlabelled sequence data, by re-writing the network to send the output back as the input to the next step, which is an example of self-supervised learning, which is where we use an unlabelled dataset to frame a supervised learning problem. This can be used to train language models, or generative music models for example. In practical we treat this case the same as a supervised learning problem, where the outputs are the same as the inputs but shifted by one time step. This particular technique is also sometimes referred to as teacher forcing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import string\n",
    "from collections import Counter, OrderedDict\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the text file into a string\n",
    "\n",
    "with open(Path('../../datasets/Shakespeare.txt'), 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a lit of chunks of text\n",
    "\n",
    "text_chunks = text.split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou art not noble;\n",
      "for all the accommodations that thou bear'st\n",
      "are nursed by baseness\n",
      "-----\n",
      "you are plebeians,\n",
      "if they be senators: and they are no less,\n",
      "when, both your voices blended, the great'st taste\n",
      "most palates theirs\n",
      "-----\n",
      "hortensio:\n",
      "who shall begin?\n",
      "\n",
      "lucentio:\n",
      "that will i\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Display some randomly selected text samples\n",
    "\n",
    "num_samples = 3\n",
    "indices = np.random.choice(len(text_chunks), num_samples, replace=False)\n",
    "for chunk in np.array(text_chunks)[indices]:\n",
    "    print(chunk)\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip any whitespace at the beginning or end of the strings and convert the strings to lowercase\n",
    "\n",
    "text_chunks = [s.strip().lower() for s in text_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the chunks that are too short or too long\n",
    "\n",
    "text_chunks = [sentence for sentence in text_chunks if 10 <= len(sentence) <= 400]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create text inputs and targets\n",
    "def create_pure_inputs_and_targets(chunks):\n",
    "    inputs = [chunk[:-1] for chunk in chunks]\n",
    "    targets = [chunk[1:] for chunk in chunks]\n",
    "\n",
    "    return list(zip(inputs, targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pure inputs and targets\n",
    "\n",
    "pure_ds = create_pure_inputs_and_targets(text_chunks)\n",
    "\n",
    "# Make train and validation splits\n",
    "\n",
    "train_set, validation_set = train_test_split(pure_ds, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that converts the sentences to tokens at the character level\n",
    "\n",
    "def get_vocab(chunks):\n",
    "    counter = Counter(''.join(chunks))\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    vocab = torchtext.vocab.vocab(ordered_dict, specials=['<unk>']) # special characters\n",
    "    vocab.set_default_index(-1) # token for unk characters\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "\n",
    "vocab = get_vocab(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that preprocesses the data and return dataloaders\n",
    "def get_loaders(train_set, validation_set, batch_size):\n",
    "    def collate_batch(batch):\n",
    "        input_list, target_list = [], []\n",
    "        for input, target in batch:\n",
    "            processed_input = torch.tensor([vocab[c] for c in input], dtype=torch.int64)\n",
    "            processed_target = torch.tensor([vocab[c] for c in target], dtype=torch.int64)\n",
    "\n",
    "            input_list.append(processed_input)\n",
    "            target_list.append(processed_target)\n",
    "        \n",
    "        # input sequence의 길이가 다를 때 이를 동일하게 맞추어 주기 위하여 padding 추가\n",
    "        input_tensor = torch.nn.utils.rnn.pad_sequence(input_list, batch_first=True, padding_value=0)\n",
    "        target_tensor = torch.nn.utils.rnn.pad_sequence(target_list, batch_first=True, padding_value=0)\n",
    "        return input_tensor.to(device), target_tensor.to(device)\n",
    "    \n",
    "    # collate_fn: Batch를 특정한 처리를 하여 결합하고자 할 때 사용하는 함수\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=batch_size, collate_fn=collate_batch)\n",
    "\n",
    "    return train_loader, validation_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, validation_loader = get_loaders(train_set, validation_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a RNN model\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab, embedding_dim, gru_units):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(input_size=embedding_dim, hidden_size=gru_units, batch_first=True)\n",
    "        self.linear = nn.Linear(gru_units, len(vocab))\n",
    "    \n",
    "    def forward(self, x, h0=None):\n",
    "        x = self.embedding(x)\n",
    "        out_gru, h = self.gru(x, h0)\n",
    "        out = self.linear(out_gru)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embedding): Embedding(39, 256, padding_idx=0)\n",
       "  (gru): GRU(256, 1024, batch_first=True)\n",
       "  (linear): Linear(in_features=1024, out_features=39, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model = RNN(vocab, 256, 1024).to(device)\n",
    "rnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an EarlyStopping class for training\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.min_valid_loss = np.inf\n",
    "    \n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_valid_loss:\n",
    "            self.min_valid_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > self.min_valid_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loss_function, optimiser, train_loader, validation_loader, early_stopping, epochs):\n",
    "    epoch_losses, epoch_losses_validation = [], []\n",
    "    epoch_acc, epoch_acc_validation = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        sum_loss, sum_loss_validation = 0., 0.\n",
    "        sum_acc, sum_acc_validation = 0., 0.\n",
    "\n",
    "        for inputs, y_true in train_loader:\n",
    "            optimiser.zero_grad()\n",
    "\n",
    "            y_pred = model(inputs)[0]\n",
    "            loss = loss_function(y_pred, F.one_hot(y_true, len(vocab)).float())\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "\n",
    "            sum_loss += loss.item()\n",
    "            sum_acc += (y_true == y_pred.argmax(dim=2)).sum() / (inputs.shape[0] * inputs.shape[1])\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y_true in validation_loader:\n",
    "                y_pred = model(inputs)[0]\n",
    "\n",
    "                loss = loss_function(y_pred, F.one_hot(y_true, len(vocab)).float())\n",
    "                sum_loss_validation += loss.item()\n",
    "                sum_acc_validation += (y_true == y_pred.argmax(dim=2)).sum() / (inputs.shape[0] * inputs.shape[1])\n",
    "        \n",
    "        avg_epoch_loss = sum_loss / len(train_loader)\n",
    "        avg_epoch_acc = sum_acc / len(train_loader)\n",
    "\n",
    "        avg_epoch_loss_validation = sum_loss_validation / len(validation_loader)\n",
    "        avg_epoch_acc_validation = sum_acc_validation / len(validation_loader)\n",
    "\n",
    "        epoch_losses.append(avg_epoch_loss)\n",
    "        epoch_acc.append(avg_epoch_acc)\n",
    "\n",
    "        epoch_losses_validation.append(avg_epoch_loss_validation)\n",
    "        epoch_acc_validation.append(avg_epoch_acc_validation)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - loss: {avg_epoch_loss:.4f}, val_loss: {avg_epoch_loss_validation:.4f}, \"\n",
    "              f'accuracy: {avg_epoch_acc:.4f}, val_accuracy: {avg_epoch_acc_validation:.4f}')\n",
    "        \n",
    "        if early_stopping.early_stop(avg_epoch_loss_validation):\n",
    "            break\n",
    "\n",
    "    history = {\n",
    "        'loss': epoch_losses,\n",
    "        'val_loss': epoch_losses_validation,\n",
    "        'accuracy': epoch_acc,\n",
    "        'val_accuracy': epoch_acc_validation\n",
    "    }\n",
    "\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - loss: nan, val_loss: nan, accuracy: 0.6383, val_accuracy: 0.6459\n",
      "Epoch 2 - loss: nan, val_loss: nan, accuracy: 0.6412, val_accuracy: 0.6459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[81], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCrossEntropyLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrnn_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[80], line 16\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, loss_function, optimiser, train_loader, validation_loader, early_stopping, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(y_pred, F\u001b[38;5;241m.\u001b[39mone_hot(y_true, \u001b[38;5;28mlen\u001b[39m(vocab))\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m---> 16\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m optimiser\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     19\u001b[0m sum_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "history = train_model(rnn_model, nn.CrossEntropyLoss(), torch.optim.Adam(rnn_model.parameters()), train_loader, validation_loader, EarlyStopping(patience=3), epochs=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
