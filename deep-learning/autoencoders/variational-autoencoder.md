# Variational Autoencoder

### Preliminaries

One often cares about producing more examples that are like those already in a database, but not exactly the same. We get examples $X$ distributed according to some unknown distribution $P_{gt}(X)$(gt: ground truth), and our goal is to learn a model $P$ which we can sample from, such that $P$ is as similar as possible to $P_{gt}$. Most of the approaches from the past had three serious drawbacks: 

- They require strong assumptions about the structure in the data. 
- They make severe approximations, leading to sub-optimal models.
- They rely on computationally expensive inference procedures like Markov Chain Monte Carlo.

Backpropagation-based function approximators are used nowadays as an alternative. Variational autoencoders have weak assumptions and can be trained fast thanks to backpropagation.

#### Latent Variable Models

Before we can say that our model is representative of our dataset, we need to make sure that for every datapoint $X$ in the dataset, there is one  (or many) settings of the latent variables which causes the model to generate something very similar to $X$. Formally, say we have a vector of latent variables $z$ in a high-dimensional space $\mathcal Z$ which we can easily sample according to some probability density function(pdf) $P(z)$ defined over $\mathcal Z$. Then, say we have a family of deterministic functions $f(z;\theta)$, parameterised by a vector $\theta$ in some space $\Theta$, where $f:\mathcal Z\times \Theta \to \mathcal X$. We have $f$ deterministic, but if $z$ is random and $\theta$ is fixed, then $f(z;\theta)$ is a random variable in the space $\mathcal X$. We wish to optimise $\theta$ such that we can sample $z$ from $P(z)$ and, with high probability, $f(z;\theta)$ will be like the $X$’s in our dataset. Mathematically, we are aiming to maximise the probability of each $X$ in the training set under the entire generative process, according to:

$$
P(X)=\int P(X\mid z;\theta)P(z)dz.
$$

where we replaced $f(z;\theta)$ by a distribution $P(X\mid z;\theta)$, which allows us to make the dependence of $X$ on $z$ explicit by using the law of total probability. Here, $P(z)$ is the probability of $z$ being set to a particular value, and $P(X\mid z;\theta)$ is the probability of the model coming up with an output $X$ given $z$ and $\theta$. In VAEs, the choice of this output distribution is often Gaussian, i.e., $P(X\mid z;\theta)=\mathcal N(X\mid f(z;\theta), \sigma^2 I)$. It has mean $f(z;\theta)$ and covariance equal to the identity matrix $I$ times some scalar $\sigma$ (which is a hyperparameter). This replacement is necessary to formalise the intuition that some $z$ needs to result in samples that are merely like $X$.

### Variational Autoencoders

To solve 

$$
P(X)=\int P(X\mid z;\theta)P(z)dz
$$

there are two problems that VAEs must deal with:
- how to define the latent variables $z$ (i.e., decide what information they represent)
- how to deal with the integral over $z$.

VAEs assume that there is no simple interpretation of the dimensions of $z$, and instead assert that samples of $z$ can be drawn from a simple distribution, i.e., $\mathcal N(0,I)$, where $I$ is the identity matrix. Because any distribution in $d$ dimensions can be generated by taking a set of $d$ variables that are normally distributed and mapping them through a sufficiently complicated function. Hence, provided powerful function approximators, we can simply learn a function which maps our independent, normally-distributed $z$ values to whatever latent variables might be needed for the model, and then map those latent variables to $X$.

In practice, for most $z$, $P(X\mid z)$ will be nearly zero, and hence contribute almost nothing to our estimate of $P(X)$. The key idea behind the variational autoencoder is to attempt to sample values of $z$ that are likely to have produced $X$, and computed $P(X)$ just from those. This means that we need a new function $Q(z\mid X)$ which can take a value of $X$ and give us a distribution over $z$ values that are likely to produce $X$. Hopefully this space of $z$ values that are likely under $Q$ will be much smaller than the space of all $z$’s that are likely under the prior $P(z)$, which lets us compute $\mathbb E_{z\sim Q}\left[ P(X\mid z) \right]$ relatively easily.

Kullback-Leibler divergence (KL divergence, $\mathcal D$) between $P(z\mid X)$ and $Q(z)$ for some arbitrary $Q$ is define as

$$
\mathcal D\left[ Q(z)\|P(z\mid X) \right] = \mathbb E_{z\sim Q}\left[ \log Q(z) - \log P(z\mid X) \right].
$$

Applying Bayes rule to $P(z\mid X)$, we have

$$
\mathcal D\left[ Q(z)\| P(z\mid X) \right] = \mathbb E_{z\sim Q}\left[ \log Q(z) - \log P(X\mid z) - \log P(z) \right] + \log P(X).
$$

Then we have

$$
\log P(X) - \mathcal D\left[ Q(z)\| P(z\mid X) \right] = \mathbb E_{z\sim Q}\left[ \log P(X\mid z)    \right] - \mathcal D\left[ Q(z)\| P(z) \right].
$$

Since we are interested in inferring $P(X)$, it makes sense to construct a $Q$ which does depend on $X$, and in particular, one which makes $\mathcal D\left[ Q(z)\| P(z\mid X) \right]$ small:

$$
\log P(X) - \mathcal D\left[ Q(z\mid X)\| P(z\mid X) \right] = \mathbb E_{z\sim Q}\left[ \log P(X\mid z)    \right] - \mathcal D\left[ Q(z\mid X)\| P(z) \right].
$$

The left hand side has the quantity we want to maximise. If $Q$ is high-capacity, $Q$ can produce $z$’s that can reproduce a given $X$ and the error term $\mathcal D\left[ Q(z\mid X)\| P(z\mid X) \right]$ will become small. The right hand side is something we can optimise via stochastic gradient descent given the right choice of $Q$. Now the right hand side looks similar with an autoencoder, as $Q$ is “encoding” $X$ into $z$ and $P$ is “decoding” it to reconstruct $X$. Note that here $P(z\mid X)$ is not something we can compute analytically as it describes the values of $z$ that are likely to give rise to a sample like $X$ under our model. Instead, we have made the intractable $P(z\mid X)$ tractable: we can just use $Q(z\mid X)$ to compute it.

The usual choice of $Q(z\mid X)$ is

$$
Q(z\mid X) = \mathcal N(z \mid \mu(X;\theta),\Sigma(X;\theta))
$$

where $\mu$ and $\Sigma$ are arbitrary deterministic functions with parameters $\theta$ that can be learned from data. In practice, $\mu$ and $\Sigma$ are implemented via neural networks, and $\Sigma$ is constrianed to be a diagonal matrix.

The term $\mathcal D\left[ Q(z\mid X)\| P(z) \right]$ is now a KL-divergence between two multivariate Gaussian distributions, which can be computed in a closed form as:

$$
\mathcal D\left[ \mathcal N(\mu_0, \Sigma_0)\| \mathcal N(\mu_1,\Sigma_1) \right] = \frac{1}{2} \left( \text{tr}\left( \Sigma_1^{-1}\Sigma_0 \right) + (\mu_1-\mu_0)^\intercal (\mu_1 - \mu_0) - k - \log \left( \frac{\det \Sigma_1}{\det \Sigma_0} \right) \right)
$$

where $k$ is the dimensionality of the distribution. As we have $P(z)=\mathcal N(0, 1)$, we have

$$
\mathcal D\left[ \mathcal N(\mu_0, \Sigma_0)\| \mathcal N(\mu_1,\Sigma_1) \right] 
= 
\frac{1}{2} \left( 
\text{tr}\left( \Sigma(X)\right) 
+ (\mu(X))^\intercal (\mu(X)) 
- k 
- \log \det(\Sigma(X))\right)
$$

On the right hand side, it might be costly to use sampling to estimate the first term. Therefore, as is standard in stochastic gradient descent, we take one sample of $z$ and treat $\log P(X\mid z)$ for that $z$ as an approximation of $\mathbb E_{z\sim Q}\left[ \log P(X\mid z) \right]$. The equation we want to optimise is:

$$
\mathbb E_{X\sim D}\left[ \log P(X) - \mathcal D\left[ Q(z\mid X)\| P(z\mid X) \right] \right] = \mathbb E_{X\sim D}\left[ \mathbb E_{z\sim Q}\left[ \log P(X\mid z) \right] - \mathcal D\left[ Q(z\mid X)\| P(z) \right] \right].
$$

We can sample a single value of $X$ and a single value of $z$ from the distribution $Q(z\mid X)$ and compute the gradient of 

$$
\log P(X\mid z)- \mathcal D\left[ Q(z\mid X)\| P(z) \right].
$$

Then we average the gradient of this function over arbitrarily many samples of $X$ and $z$ and the result converges to the gradient of $\mathbb E_{X\sim D}\left[ \log P(X) - \mathcal D\left[ Q(z\mid X)\| P(z\mid X) \right] \right]$. However, this way, $\log P(X\mid z)- \mathcal D\left[ Q(z\mid X)\| P(z) \right]$ cannot backpropagate the error through a layer that samples $z$ from $Q(z\mid X)$, which is a non-continuous operation and has no gradient.

The solution to the above problem is to move the sampling to an input layer. Given $\mu(X)$ and $\Sigma(X)$—the mean and covariance of $Q(z\mid X)$—we can sample from $\mathcal N(\mu(X), \Sigma(X))$ by first sampling $\epsilon \sim \mathcal N(0,I)$, then computing $z=\mu(X)+\Sigma^{1/2}(X)\cdot \epsilon$. Thus, the equation we actually take the gradient of is:

$$
\mathbb E_{X\sim D}\left[ \mathbb E_{\epsilon\sim \mathcal N(0,1)}\left[ \log P(X\mid z = \mu(X) + \Sigma^{1/2}(X)\cdot \epsilon) \right] - \mathcal D\left[ Q(z\mid X)\| P(z) \right] \right].
$$