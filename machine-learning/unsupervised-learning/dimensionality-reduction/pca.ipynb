{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Principal Component Analysis\n",
    "\n",
    "Consider there is a $q$-dimensional random vector\n",
    "\n",
    "$$\n",
    "X \\sim (m, \\Sigma)\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\begin{gather*}\n",
    "m=\\mathbb E[X]=\\begin{bmatrix}\n",
    "m_1 \\\\ \\vdots \\\\ m_q\\end{bmatrix}\n",
    "\\\\\n",
    "\\Sigma=\\text{Var}[X]=\\begin{bmatrix} \\Sigma_{11}&\\cdots&\\Sigma_{1q}\\\\ \\vdots&\\ddots&\\vdots\n",
    "\\\\\n",
    "\\Sigma_{q1}&\\cdots&\\Sigma_{qq}\n",
    "\\end{bmatrix}\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "If we were to approximate $X$ through a single straight line, one approach would be to choose a line that minimises the expected squared distances between $X$ and their projections $X'$ onto the line. \n",
    "\n",
    "![pca-projection](../../../figures/pca-projection.jpeg)\n",
    "\n",
    "Pythagorean theorem gives\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb E[\\overline{XX'}^2]=\\mathbb E[\\overline{mX}^2]-\\mathbb E[\\overline{mX'}^2]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\overline{XX'}=\\|X-X'\\|$. As $\\mathbb E[\\overline{mX}^2]$ does not depend on the fitted line, minimising $\\mathbb E[\\overline{XX'}^2]$ is the same as maximising $\\mathbb E[\\overline{mX'}^2]$.\n",
    "\n",
    "Investigating this quantity further, let $\\gamma$ be a unit vector pointing in the direction of the vector $X'-m$ and let $t\\in\\R$ be a scalar such that the projected point is given by\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "X'=m+t\\gamma \\in\\R^q.\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "The Euclidean distance between $m$ and $X'$, $\\overline{mX'}$, equals $|t|$. We have by definition\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\cos(\\gamma,X-m)=\\frac{t}{\\|X-m\\|}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and using the properties of the dot product gives\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\gamma^T(X-m)=\\|\\gamma\\|\\cdot\\|X-m\\|\\cos(\\gamma,X-m)=t.\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "To maximise $\\mathbb E[\\overline{mX'}^2]$, we have\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb E[\\overline{mX'}^2]&=\\mathbb E[\\overline{mX'}\\cdot \\overline{mX'}]\n",
    "\\\\\n",
    "&=\\mathbb E[\\gamma^T(X-m)(X-m)^T\\gamma]\n",
    "\\\\\n",
    "&= \\text{Var}[\\gamma^T(X-m)]\n",
    "\\\\\n",
    "&= \\text{Var}[\\gamma^T X]\n",
    "\\\\\n",
    "&= \\gamma^T \\text{Var}[X] \\gamma\n",
    "\\\\\n",
    "&= \\gamma^T\\Sigma\\gamma.\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "Hence, we need to maximise $\\gamma^T\\Sigma\\gamma$ under the constraint $\\|\\gamma\\|^2=\\gamma^T\\gamma=1$. This is a constrained maximisation problem which can be addressed through the use of a Lagrange multiplier. Define\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "P(\\gamma)=\\gamma^T\\Sigma\\gamma-\\lambda(\\gamma^T\\gamma-1).\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "Then\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial P}{\\partial \\gamma}=2\\Sigma\\gamma-2\\lambda \\gamma\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "Setting this equal to zero yields\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma\\gamma=\\lambda\\gamma.\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "So, $\\gamma$ must be an eigenvector of $\\Sigma$. Multiplying the above equation with $\\gamma^T$, we have\n",
    "        \n",
    "$$\n",
    "\\begin{gather*}\n",
    "\\gamma^T\\Sigma\\gamma=\\lambda\\gamma^T\\gamma=\\lambda\n",
    "\\\\\n",
    "\\text{Var}[\\gamma^T X]=\\gamma^T\\Sigma\\gamma=\\lambda.\n",
    "\\end{gather*}\n",
    "$$\n",
    "\n",
    "whose left hand side is exactly what we want to maximise. Maximising $\\text{Var}[\\gamma^T X]$ means that we need to choose the eigenvector $\\gamma_1$ corresponding to the largest eigenvalue $\\lambda_1$. The new random variable $\\gamma_1^T X$ is the linear combination of $X=(X_1,\\cdots,X_q)^T$ with maximal variance, and is called the first principal component (PC) of $X$, and the line\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "g_1(t)=m+t\\gamma_1\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "is the corresponding first principal component line.\n",
    "        \n",
    "Similarly, we define higher-order PC’s: The $j$-th eigenvector $\\gamma_j$ maximises $\\text{Var}[\\gamma^TX]$ over all $\\gamma$ which are orthogonal to $\\gamma_1,\\cdots,\\gamma_{j-1}$. The $j$-th PC is given by $\\gamma_j^T X$ and $g_j(t)=m+t\\gamma_j$ is the corresponding $j$-th PC line. For data $Z$, we need to replace $m$ and $\\hat m$, and $\\Sigma$ by some estimate $\\hat \\Sigma$ (ML - maximum likelihood of sample), yielding $\\hat \\lambda_1,\\cdots,\\hat\\lambda_q,\\hat\\gamma_1,\\cdots,\\hat\\gamma_q$. Note that principal components minimise the (sum of squared) orthogonal distances between data $X_1,\\cdots,X_n$ and the line, unlike vertical distances as in the regression context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decomposing Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $j$-th eigenvector $\\gamma_j$ of $\\Sigma$, we have\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\Sigma\\gamma_j=\\lambda_j\\gamma_j\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "for $j=1,\\cdots,q$.\n",
    "    \n",
    "We can bind these $q$ vectors together to form $q\\times q$ matrices:\n",
    "    \n",
    "$$\n",
    "\\begin{gather*}\n",
    "(\\Sigma\\gamma_1,\\cdots,\\Sigma\\gamma_q)=(\\lambda_1\\gamma_1,\\cdots,\\lambda_q\\gamma_q)\n",
    "\\\\\n",
    "\\Sigma(\\gamma_1,\\cdots,\\gamma_q)=(\\gamma_1,\\cdots,\\gamma_q)\\begin{bmatrix}\n",
    "\\lambda_1&&\n",
    "\\\\\n",
    "&\\ddots&\n",
    "\\\\\n",
    "&&\\lambda_q\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "\\Sigma\\Gamma=\\Gamma\\Lambda\n",
    "\\\\\n",
    "\\Sigma=\\Gamma\\Lambda\\Gamma^{-1}=\\Gamma\\Lambda\\Gamma^T\n",
    "\\end{gather*}\n",
    "$$\n",
    "    \n",
    "This decomposition is called the eigen decomposition of $\\Sigma$. Now we have\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lambda_j=\\text{Var}[\\gamma_j^T X]\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "so the $\\lambda_j$ provide some decomposition of variance and each $\\lambda_j$ takes the shares of something. Computing their sum we obtain\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lambda_1+\\cdots+\\lambda_q=\\text{Tr}(\\Lambda)=\\text{Tr}(\\Gamma^T\\Sigma\\Gamma)=\\text{Tr}(\\Gamma\\Gamma^T\\Sigma)=\\text{Tr}(\\Sigma)\\equiv\\text{TV}[X].\n",
    "\\end{align*}\n",
    "$$\n",
    "        \n",
    "The trace of the variance matrix is called the total variance. Therefore,\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\lambda_j}{\\lambda_1+\\cdots+\\lambda_q}=\\frac{\\text{Var}[\\gamma_j^TX]}{\\text{TV}[X]}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "is the proportion of total variance explained by the $j$-th principal component.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "If the variables operate on different scales/units, the decomposition of variance may not be meaningful as it depends on the units chosen. To avoid problems of this type, we standardise all variables by dividing through their standard deviation\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\tilde X_j=\\frac{X_j}{\\text{SD}[X_j]}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "and then apply PCA to $\\tilde X=(\\tilde X_1,\\cdots,\\tilde X_q)^T$.\n",
    "    \n",
    "Observe that for eigenvalues of $\\lambda_1,\\cdots,\\lambda_q$ of the variance matrix of $\\tilde X$,\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\lambda_1+\\cdots+\\lambda_q=\\text{Tr}(\\text{Var}[\\tilde X])=1+\\cdots+1=q\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "meaning that the eigenvalues of the correlation matrix always add up to the dimension of the random vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Compression (Dimensionality Reduction)\n",
    "\n",
    "Let $x_1,\\cdots,x_n$ be sampled from the $q$-dimensional random variable $X\\sim (m,\\Sigma)$ yielding a data matrix (or data frame)\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "\\begin{bmatrix}\n",
    "x_1^T \\\\ \\vdots \\\\ x_n^T\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x_{11} & \\cdots & x_{1q}\n",
    "\\\\\n",
    "\\vdots & \\ddots & \\vdots\n",
    "\\\\\n",
    "x_{n1} & \\cdots & x_{nq}\n",
    "\\end{bmatrix}\n",
    "\\in \\R^{n\\times q}.\n",
    "\\end{align*}\n",
    "$$\n",
    "    \n",
    "Assume a PCA has been carried out as usual, yielding $m$, $\\Sigma$, $\\gamma_1,\\cdots,\\gamma_q$, $\\lambda_1,\\cdots,\\lambda_q$.\n",
    "Suppose we wish to compress the data towards a smaller dimension $d\\leq q$. The scree plot usually can be split into a left-hand part that contains the components which capture significant variation in the data, and a right-hand part which just contains noise or scree. We cut off where this breakpoint, also called “knee”, is deemed to be situated\n",
    "        \n",
    "![pca-scree](../../../figures/pca-scree.png)\n",
    "        \n",
    "Data compression means projection. We project all data points $x_i$ for $i=1,\\cdots,n$ onto the $d$-dimensional subspace by the $d$ largest principal components:\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "f:\\R^q\\to\\R^d,x_i\\mapsto(\\gamma_1,\\cdots,\\gamma_d)^T(x_i-m)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where the $f(x_i)\\equiv t$ are called principal component scores.\n",
    "        \n",
    "If desired, scores can be mapped back to the data space:\n",
    "        \n",
    "$$\n",
    "\\begin{align*}\n",
    "g:\\R^d\\to\\R^q, t_i\\mapsto m+(\\gamma_1,\\cdots,\\gamma_d)t_i\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "which leads to reconstructed values $r_i\\equiv(g\\circ f)(x_i)$.\n",
    "\n",
    "The original data will not be exactly reconstructed, unless $d=q$. When $d=q$, we have\n",
    "                \n",
    "$$\n",
    "\\begin{align*}\n",
    "(g\\circ f)(x_i)&=g(f(x_i))\n",
    "\\\\\n",
    "&=m+(\\gamma_1,\\cdots,\\gamma_d)(\\gamma_1,\\cdots,\\gamma_d)^T(x_i-m)\n",
    "\\\\\n",
    "&=m+\\Gamma\\Gamma^T(x_i-m)\n",
    "\\\\\n",
    "&= m+x_i-m\n",
    "\\\\\n",
    "&= x_i.\n",
    "\\end{align*}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
