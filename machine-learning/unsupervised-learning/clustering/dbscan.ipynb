{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN: Density-Based Spatial Clustering of Applications with Noise - A Deeper Dive\n",
    "\n",
    "## Introduction\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a non-parametric, density-based clustering algorithm that excels in discovering clusters of arbitrary shapes and sizes in spatial datasets while effectively handling noise. Unlike partitioning-based algorithms like k-means, which assume clusters are spherical and require pre-specification of the number of clusters, DBSCAN leverages the concept of density connectivity to identify clusters based on local density variations. This makes it particularly valuable for real-world datasets where clusters often exhibit complex, non-linear structures and contain significant noise.\n",
    "\n",
    "## Core Concepts - A Formal Treatment\n",
    "\n",
    "1.  **Epsilon (ε) or Radius:** This parameter defines the radius of the neighborhood around a given point. Mathematically, the ε-neighborhood of a point $x_i$ is defined as:\n",
    "    $$N_\\epsilon(x_i) = \\{x_j \\in D \\mid dist(x_i, x_j) \\le \\epsilon\\}$$\n",
    "    where $D$ is the dataset and $dist(x_i, x_j)$ represents the distance between points $x_i$ and $x_j$ using a chosen distance metric (e.g., Euclidean, Manhattan, or Minkowski distance). The choice of distance metric is crucial and should be tailored to the nature of the data.\n",
    "\n",
    "2.  **MinPts:** This parameter specifies the minimum number of points required within the ε-neighborhood for a point to be considered a core point. A higher MinPts value increases the robustness of the algorithm to noise but may also lead to the loss of smaller, denser clusters.\n",
    "\n",
    "3.  **Core Point:** A point $x_i$ is a core point if its ε-neighborhood contains at least MinPts points:\n",
    "    $$|N_\\epsilon(x_i)| \\ge MinPts$$\n",
    "    where $|N_\\epsilon(x_i)|$ denotes the cardinality (number of elements) of the ε-neighborhood of $x_i$. Core points reside in dense regions of the dataset.\n",
    "\n",
    "4.  **Border Point:** A point $x_j$ is a border point if it is within the ε-neighborhood of a core point but does not satisfy the core point condition itself:\n",
    "    $$x_j \\in N_\\epsilon(x_i) \\text{ and } |N_\\epsilon(x_j)| < MinPts \\text{ for some core point } x_i$$\n",
    "    Border points lie on the edge of a cluster.\n",
    "\n",
    "5.  **Noise Point (Outlier):** A point $x_k$ is a noise point if it is neither a core point nor a border point:\n",
    "    $$|N_\\epsilon(x_k)| < MinPts \\text{ and } x_k \\notin N_\\epsilon(x_i) \\text{ for any core point } x_i$$\n",
    "    Noise points are isolated points that do not belong to any cluster.\n",
    "\n",
    "## Density Reachability and Connectivity\n",
    "\n",
    "1.  **Directly Density-Reachable:** A point $x_j$ is directly density-reachable from $x_i$ if $x_i$ is a core point and $x_j$ is within the ε-neighborhood of $x_i$:\n",
    "    $$x_j \\in N_\\epsilon(x_i) \\text{ and } |N_\\epsilon(x_i)| \\ge MinPts$$\n",
    "\n",
    "2.  **Density-Reachable:** A point $x_j$ is density-reachable from $x_i$ if there exists a chain of points $p_1, p_2, ..., p_n$, where $p_1 = x_i$ and $p_n = x_j$, such that $p_{i+1}$ is directly density-reachable from $p_i$ for all $i = 1, 2, ..., n-1$. This establishes a transitive relationship between points within a cluster.\n",
    "\n",
    "3.  **Density-Connected:** Two points $x_i$ and $x_j$ are density-connected if there exists a point $x_k$ such that both $x_i$ and $x_j$ are density-reachable from $x_k$. Density connectivity implies that $x_i$ and $x_j$ belong to the same cluster.\n",
    "\n",
    "## DBSCAN Algorithm - Step-by-Step\n",
    "\n",
    "1.  **Initialization:**\n",
    "    -   Select appropriate values for ε and MinPts based on the dataset characteristics.\n",
    "    -   Mark all points as unvisited.\n",
    "\n",
    "2.  **Iteration:**\n",
    "    -   For each unvisited point $x_i$ in the dataset:\n",
    "        -   Mark $x_i$ as visited.\n",
    "        -   Retrieve the ε-neighborhood of $x_i$, $N_\\epsilon(x_i)$.\n",
    "        -   If $|N_\\epsilon(x_i)| < MinPts$, mark $x_i$ as noise and proceed to the next unvisited point.\n",
    "        -   Otherwise, create a new cluster $C$ and add $x_i$ to $C$.\n",
    "        -   Initiate cluster expansion by adding all density-reachable points from $x_i$ to $C$.\n",
    "\n",
    "3.  **Cluster Expansion:**\n",
    "    -   Create a queue $Q$ and add all points in $N_\\epsilon(x_i)$ to $Q$.\n",
    "    -   While $Q$ is not empty:\n",
    "        -   Remove a point $x_j$ from $Q$.\n",
    "        -   If $x_j$ is unvisited:\n",
    "            -   Mark $x_j$ as visited.\n",
    "            -   Retrieve the ε-neighborhood of $x_j$, $N_\\epsilon(x_j)$.\n",
    "            -   If $|N_\\epsilon(x_j)| \\ge MinPts$, add all points in $N_\\epsilon(x_j)$ to $Q$.\n",
    "        -   If $x_j$ is not a member of any cluster, add $x_j$ to $C$.\n",
    "\n",
    "4.  **Repeat:** Repeat step 2 until all points in the dataset have been visited.\n",
    "\n",
    "## Parameter Selection - Practical Considerations\n",
    "\n",
    "-   **ε (Radius):**\n",
    "    -   **k-distance graph:** A common method involves plotting the k-distances (distances to the k-th nearest neighbor) for all points in the dataset, sorted in ascending order. The \"knee\" or \"elbow\" point in this graph often provides a good estimate for ε.\n",
    "    -   **Domain knowledge:** Prior knowledge about the dataset and the expected scale of clusters can also inform the selection of ε.\n",
    "-   **MinPts:**\n",
    "    -   **Heuristics:** A common heuristic is to set MinPts to $2d$, where $d$ is the dimensionality of the data.\n",
    "    -   **Experimentation:** It is often necessary to experiment with different MinPts values to find a suitable value that balances noise robustness and cluster detection.\n",
    "    -   **Rule of thumb:** MinPts >= d + 1.\n",
    "\n",
    "## Advantages - Beyond the Basics\n",
    "\n",
    "-   **Robustness to noise:** DBSCAN effectively identifies and handles outliers, preventing them from influencing cluster formation.\n",
    "-   **Arbitrary cluster shapes:** DBSCAN can discover clusters of any shape, including non-convex and intertwined clusters.\n",
    "-   **No need to specify the number of clusters:** DBSCAN automatically determines the number of clusters based on the data density.\n",
    "\n",
    "## Disadvantages - Addressing Limitations\n",
    "\n",
    "-   **Sensitivity to parameters:** The performance of DBSCAN is highly dependent on the choice of ε and MinPts.\n",
    "-   **Varying densities:** DBSCAN struggles with datasets where clusters have significantly different densities.\n",
    "-   **Computational complexity:** The time complexity of DBSCAN is $O(n^2)$ in the worst case, where $n$ is the number of data points. Optimized implementations using spatial indexing structures (e.g., k-d trees, ball trees) can reduce the complexity to $O(n \\log n)$.\n",
    "-   **High-dimensional data:** In high-dimensional spaces, the curse of dimensionality can make it difficult to define meaningful density thresholds.\n",
    "\n",
    "## Extensions and Variations\n",
    "\n",
    "-   **HDBSCAN (Hierarchical DBSCAN):** Addresses the issue of varying densities by converting DBSCAN into a hierarchical clustering algorithm.\n",
    "-   **OPTICS (Ordering Points To Identify the Clustering Structure):** Creates an ordering of the data points that represents the density-based clustering structure, allowing for the extraction of clusters with varying densities.\n",
    "-   **DBSCAN with different distance metrics:** Adapting DBSCAN to use non-Euclidean distance metrics, such as cosine similarity or Jaccard distance, can improve its performance in specific domains.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "DBSCAN is a powerful and versatile clustering algorithm that offers significant advantages over traditional partitioning-based methods. By understanding its mathematical foundations, parameter selection considerations, and limitations, postgraduate statistics students can effectively apply DBSCAN to a wide range of real-world datasets and gain valuable insights from complex data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.datasets import make_moons, make_blobs\n",
    "\n",
    "def visualize_dbscan(data, eps, min_samples, title):\n",
    "    \"\"\"\n",
    "    Applies DBSCAN and visualizes the clustering results.\n",
    "\n",
    "    Args:\n",
    "        data (numpy.ndarray): The dataset to cluster.\n",
    "        eps (float): The maximum distance between two samples for one to be considered as in the neighborhood of the other.\n",
    "        min_samples (int): The number of samples (or total weight) in a neighborhood for a point to be considered as a core point.\n",
    "        title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(data)\n",
    "\n",
    "    # Number of clusters in labels, ignoring noise if present.\n",
    "    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "    n_noise_ = list(labels).count(-1)\n",
    "\n",
    "    print(f\"Estimated number of clusters: {n_clusters_}\")\n",
    "    print(f\"Estimated number of noise points: {n_noise_}\")\n",
    "\n",
    "    # Plotting the results\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each) for each in np.linspace(0, 1, len(unique_labels))]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = data[class_member_mask]\n",
    "        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col), markeredgecolor='k', markersize=6)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Example 1: Moons dataset\n",
    "X_moons, _ = make_moons(n_samples=300, noise=0.05, random_state=0)\n",
    "visualize_dbscan(X_moons, eps=0.2, min_samples=5, title=\"DBSCAN on Moons Dataset\")\n",
    "\n",
    "# Example 2: Blobs dataset\n",
    "X_blobs, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "visualize_dbscan(X_blobs, eps=0.7, min_samples=5, title=\"DBSCAN on Blobs Dataset\")\n",
    "\n",
    "# Example 3: Dataset with varying densities\n",
    "random_state = 170\n",
    "X_varied, y_varied = make_blobs(n_samples=300, centers=8, random_state=random_state, centers=[[1, 1], [-1, -1], [1, -1],[-1,1],[3,3],[-3,-3],[3,-3],[-3,3]], cluster_std=[0.2, 1, 0.5, 0.8,0.3,1.2,0.7,0.9])\n",
    "visualize_dbscan(X_varied, eps=0.4, min_samples=5, title=\"DBSCAN on Varied Density Dataset\")\n",
    "\n",
    "# Example 4: Dataset with noise\n",
    "X_noisy, _ = make_blobs(n_samples=300, centers=3, cluster_std=0.7, random_state=0)\n",
    "noise = np.random.rand(50,2)*5 -2.5\n",
    "X_noisy = np.concatenate((X_noisy, noise), axis=0)\n",
    "visualize_dbscan(X_noisy, eps=0.8, min_samples=5, title=\"DBSCAN on Noisy Blobs Dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
