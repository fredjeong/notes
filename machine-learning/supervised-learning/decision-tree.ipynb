{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Decision Trees"]},{"cell_type":"markdown","metadata":{},"source":["Decision trees are a crucial algorithm for machine learning in predictive modelling. This technique has been around for decades, with modern variations such as random forest being among the most effective methods available."]},{"cell_type":"markdown","metadata":{},"source":["## Classification and Regression Trees"]},{"cell_type":"markdown","metadata":{},"source":["Classification and Regression Trees (CART) is an algorithm which can be used for both classification and regression problems in predictive modelling. The CART algorithm forms the basis for other advanced algorithms such as bagged decision trees, random forest, and boosted decision trees.\n","\n","The representation of the CART model is a binary tree, which can be thought of as a simple binary tree from algorithms and data structures. The root node represents a single input variable and its split point, while the leaf nodes contain the output variable used for predictions. For instance, if a dataset has two inputs, height and weight, and the output is sex (male or female), a simple binary decision tree would look like:\n","\n","![decision-tree-example](../../figures/decision-tree-example.png)\n","\n","With this binary tree representation of the CART model, making predictions is relatively straightforward. For a new input, the tree is traversed by starting at the root node and evaluating the specific input. A learned binary tree can be thought of as sa partitioning of thei nput space, with each input variable representing a dimension. The decision tree splits this space into rectangles (when there are two input variables) or hyper-rectangles (for more inputs). New data filters through the tree and lands in one of these rectangles, and the output value for that rectangle is the prediction made by the model, giving a boxy decision boundary.\n","\n","For exampl,e given the input of `height = 190cm` and `weight = 90kg`, we would traverse the above tree as follows:\n","    - `Height > 180cm: Yes`\n","    - `Weight > 80kg: Yes`\n","    - `Therefore: Male`\n","\n","It is very important to note some properties of Decision Trees:\n","\n","$$\n","\\begin{gather}\n","\\bigcap_{i=1}^n D_i = \\empty\n","\\\\\n","\\bigcup_{i=1}^n D_i = D\n","\\end{gather}\n","$$\n","\n","where $D$ is the parent node and $D_i$ for $i=1,\\cdots,n$ are child nodes.\n","\n","Decision Trees and CART are simple to understand and interpret, and can handle both numerical and categorical variables. They are also robus to outliers and can handle non-linear relationships between the input and targe tvariables. However, they can be prone to overfitting, especially if the tree is allowed to grow too deep, so various techniques, such as pruning or ensemble methods like Random Forest, are used to improve their performance."]},{"cell_type":"markdown","metadata":{},"source":["## Strategy of Greedy Algorithm"]},{"cell_type":"markdown","metadata":{},"source":["Creating a CART model involves choosing input variables and the best split points on those variables to form a tree structure. This process is accomplished using a greedy algorithm that minimises a cost function. The tree construction stops when a predetermined stopping criterion is reached, such as a minimum number of training examples in each leaf node.\n","\n","Building a binary decision tree involves dividing the input space using a technique called recursive binary splitting, which involves trying and testing different split points using a cost function. The split with the lowest cost is selected for each input variable and all possible split points.\n","\n","For regression predictive modelling problems, the cost function minimised in selecting split points is the sum of the squared errors between the actual output values and the predicted values for all training examples within a rectangle. This cost function is calculated as the $\\sum (y - \\hat y)^2$, where $y$ is the actual output for a training example and $\\hat y$ is the predicted output for that example.\n","\n","For classification the Gini index function and Entropy function can be used which provide an indication of how \"pure\" the leaf nodes are (how mixed the training data assigned to each node is). The Entropy function $E$ is defined as:\n","\n","$$\n","\\begin{align}\n","E = \\sum_{i=1}^K (-f_i \\cdot \\log f_i)\n","\\end{align}\n","$$\n","\n","where $E$ is the entropy over all classes, $f_i$ are the proportion (or frequency) of training instances with label $i$ in the rectangle of interest. We can then calculate the information gain by\n","\n","$$\n","\\begin{align}\n","IG(\\text{parent}, \\text{left} \\text{right}) = E(\\text{parent}) - \\left( n_1 E(\\text{left}) + n_2 E(\\text{right}) \\right)\n","\\end{align}\n","$$\n","\n","where $n_1 E(\\text{left}) + n_2 E(\\text{right})$ is the weighted average of entropy for each node. In the greedy algorithm every time we choose the attribute to split on which can result in the least disorder (entropy) and the most information gain. \n","\n","Consider an example where we are building a decision tree to predict whether a loan given to a person would result in a write-off or not. Our entire population consists of 30 instances. 16 belong to the write-off class and the other 14 belong to the non-write-off class. We have two features, namely `Balance` that can take on two values: `<50K` or `>50K` and `Residence` that can take on three values `OWN`, `RENT`, or `OTHER`. \n","\n","![decision-tree-example-2](../../figures/decision-tree-example-2.png)\n","\n","For the first feature `Balance` we have\n","\n","$$\n","\\begin{gather}\n","E(text{parent}) = -\\frac{16}{30}\\log\\left( \\frac{16}{30} \\right) - \\frac{14}{30}\\log\\left( \\frac{14}{30} \\right) \\approx 0.99\n","\\\\\n","E(\\text{Balance>50K}) = -\\frac{12}{13}\\log\\left( \\frac{12}{13} \\right) - \\frac{1}{13}\\log\\left( \\frac{1}{13} \\right) \\approx 0.39\n","\\\\\n","E(\\text{Balance<50K}) = -\\frac{4}{17}\\log\\left( \\frac{4}{17} \\right) - \\frac{13}{17}\\log\\left( \\frac{13}{17} \\right) \\approx 0.79\n","\\\\\n","E(\\text{Balance}) = \\frac{13}{30} \\times 0.39 + \\frac{17}{30} \\times 0.79 = 0.62\n","\\\\\n","IG(\\text{Parent}, \\text{Balance}) = E(\\text{Parent}) - E(\\text{Balance}) = 0.99 - 0.62 = 0.37\n","\\end{gather}\n","$$"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
